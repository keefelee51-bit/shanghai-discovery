name: Daily Content Pipeline

on:
  # schedule:
  #   - cron: '0 2 */5 * *'  # Uncomment for fully automatic daily runs
  workflow_dispatch:  # Manual trigger from GitHub Actions tab

jobs:
  scrape-and-process:
    runs-on: self-hosted

    defaults:
      run:
        shell: bash

    steps:
      # Windows self-hosted runner: add Git Bash to PATH so all bash steps work
      - name: Add Git Bash to PATH
        shell: cmd
        run: echo C:\Program Files\Git\bin>> %GITHUB_PATH%

      # ══════════════════════════════════════════════════════════════
      # PHASE 1 — Minimum setup needed to run the scraper
      # Goal: get to scraping as fast as possible so a bad cookie
      # fails in ~2 minutes instead of ~12.
      # ══════════════════════════════════════════════════════════════

      - name: Checkout repository
        uses: actions/checkout@v4

      # ══════════════════════════════════════════════════════════════
      # PHASE 0 — Validate all API credentials before doing any work
      # Fails fast (< 30s) if any key is missing or invalid,
      # before wasting time on scraping or heavy installs.
      # ══════════════════════════════════════════════════════════════

      - name: Validate API credentials
        env:
          VITE_ANTHROPIC_API_KEY: ${{ secrets.VITE_ANTHROPIC_API_KEY }}
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
        run: |
          FAILED=0

          # Anthropic
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://api.anthropic.com/v1/messages \
            -H "x-api-key: $VITE_ANTHROPIC_API_KEY" \
            -H "anthropic-version: 2023-06-01" \
            -H "content-type: application/json" \
            -d '{"model":"claude-haiku-4-5-20251001","max_tokens":1,"messages":[{"role":"user","content":"hi"}]}')
          if [ "$STATUS" = "200" ]; then echo "✓ Anthropic API key valid"
          else echo "✗ Anthropic API key invalid (HTTP $STATUS)"; FAILED=1; fi

          # Supabase
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$VITE_SUPABASE_URL/rest/v1/Post?limit=1" \
            -H "apikey: $VITE_SUPABASE_ANON_KEY")
          if [ "$STATUS" = "200" ]; then echo "✓ Supabase credentials valid"
          else echo "✗ Supabase credentials invalid (HTTP $STATUS)"; FAILED=1; fi

          # OpenAI (used for Whisper transcription)
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://api.openai.com/v1/models \
            -H "Authorization: Bearer $OPENAI_API_KEY")
          if [ "$STATUS" = "200" ]; then echo "✓ OpenAI API key valid"
          else echo "✗ OpenAI API key invalid (HTTP $STATUS)"; FAILED=1; fi

          # ElevenLabs
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://api.us.elevenlabs.io/v1/user \
            -H "xi-api-key: $ELEVENLABS_API_KEY")
          if [ "$STATUS" = "200" ]; then echo "✓ ElevenLabs API key valid"
          else echo "✗ ElevenLabs API key invalid (HTTP $STATUS) — video dubbing will be skipped"; fi

          # Google Vision — write to absolute path so Node.js resolves it correctly
          if [ -n "$GOOGLE_CREDENTIALS_JSON" ]; then
            echo "$GOOGLE_CREDENTIALS_JSON" > "$GITHUB_WORKSPACE/google-vision-key.json"
            if echo "$GOOGLE_CREDENTIALS_JSON" | grep -q '"type".*"service_account"'; then
              echo "✓ Google Vision credentials valid"
            else
              echo "✗ Google Vision credentials invalid — image translation will be skipped"
            fi
          else
            echo "✗ GOOGLE_CREDENTIALS_JSON not set — image translation will be skipped"
            echo "{}" > "$GITHUB_WORKSPACE/google-vision-key.json"
          fi

          if [ "$FAILED" = "1" ]; then
            echo ""
            echo "Critical credentials missing — fix secrets and re-run."
            exit 1
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      # Install only the scraper's own dependencies (no torch, no demucs yet)
      - name: Install scraper dependencies
        working-directory: scraper
        run: pip install -r requirements.txt

      # Download the Chromium browser binary — required for Playwright scraping
      - name: Install Playwright browser
        run: playwright install --with-deps chromium

      # ══════════════════════════════════════════════════════════════
      # PHASE 2 — Scrape (fail fast if cookies are bad)
      # If XHS cookies are stale the job dies here, saving all the
      # torch / npm install time below.
      # ══════════════════════════════════════════════════════════════

      - name: Run XHS scraper
        id: scrape
        working-directory: scraper
        continue-on-error: true
        env:
          COOKIES: ${{ secrets.XHS_COOKIES }}
        run: |
          echo "Starting XHS scraper..."
          python main.py --platform xhs --lt cookie --type search
          echo "XHS scraping done"
        timeout-minutes: 30

      - name: Run Weibo scraper
        id: scrape-weibo
        working-directory: scraper
        continue-on-error: true
        env:
          COOKIES: ${{ secrets.WEIBO_COOKIES }}
          CRAWLER_MAX_NOTES_COUNT: "80"
        run: |
          echo "Starting Weibo scraper..."
          python main.py --platform wb --lt cookie --type search
          echo "Weibo scraping done"
        timeout-minutes: 20

      - name: Verify Weibo data
        id: verify-weibo
        working-directory: scraper
        continue-on-error: true
        run: |
          LATEST=$(ls -t data/weibo/json/search_contents_*.json 2>/dev/null | head -n 1)
          if [ -z "$LATEST" ]; then
            echo "No Weibo data — XHS-only run"
            echo "weibo_file=" >> $GITHUB_OUTPUT
          else
            COUNT=$(node -pe "require('./$LATEST').length")
            echo "Found $COUNT Weibo posts in $LATEST"
            echo "weibo_file=$LATEST" >> $GITHUB_OUTPUT
          fi

      # Hard stop here if XHS scraping produced nothing
      - name: Verify XHS scraped data
        id: verify
        working-directory: scraper
        run: |
          LATEST_FILE=$(ls -t data/xhs/json/search_contents_*.json 2>/dev/null | head -n 1)
          if [ -z "$LATEST_FILE" ]; then
            echo "No XHS data found — cookies are likely expired. Update XHS_COOKIES secret and re-run."
            exit 1
          fi
          POST_COUNT=$(node -pe "require('./$LATEST_FILE').length")
          echo "Found $POST_COUNT posts in $LATEST_FILE"
          echo "scraped_file=$LATEST_FILE" >> $GITHUB_OUTPUT
          echo "post_count=$POST_COUNT" >> $GITHUB_OUTPUT

      # ══════════════════════════════════════════════════════════════
      # PHASE 3 — Install processing dependencies
      # Only reached if scraping succeeded, so this time is never
      # wasted on a bad-cookie run.
      # ══════════════════════════════════════════════════════════════

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Node.js dependencies
        run: npm ci

      # CPU-only torch to avoid the 2 GB CUDA download
      - name: Install audio processing dependencies
        working-directory: scraper
        run: |
          pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install demucs

      # ══════════════════════════════════════════════════════════════
      # PHASE 4 — Process and upload
      # ══════════════════════════════════════════════════════════════

      - name: Process posts with Claude API
        env:
          VITE_ANTHROPIC_API_KEY: ${{ secrets.VITE_ANTHROPIC_API_KEY }}
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
          QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/google-vision-key.json
        run: |
          XHS_FILE="./scraper/${{ steps.verify.outputs.scraped_file }}"
          WEIBO_FILE="${{ steps.verify-weibo.outputs.weibo_file }}"

          if [ -n "$WEIBO_FILE" ]; then
            echo "Processing XHS + Weibo posts (limits: 15 XHS, 10 Weibo)..."
            node scripts/process-xhs-posts.mjs "$XHS_FILE" "./scraper/$WEIBO_FILE" --limits 15,20
          else
            echo "Processing XHS-only posts (limit: 15)..."
            node scripts/process-xhs-posts.mjs "$XHS_FILE" --limit 15
          fi

      - name: Upload to Supabase
        env:
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
        run: node scripts/upload-to-supabase.mjs

      # ══════════════════════════════════════════════════════════════
      # ALWAYS — Summary + artifacts (runs even on failure)
      # ══════════════════════════════════════════════════════════════

      - name: Pipeline Summary
        if: always()
        run: |
          echo "========================================"
          echo "PIPELINE SUMMARY"
          echo "========================================"
          if [ -f "data/pipeline-output.json" ]; then
            ACCEPTED=$(node -pe "require('./data/pipeline-output.json').accepted?.length || 0")
            REJECTED=$(node -pe "require('./data/pipeline-output.json').rejected?.length || 0")
            ERRORS=$(node -pe "require('./data/pipeline-output.json').errors?.length || 0")
            echo "Scraped:  ${{ steps.verify.outputs.post_count }} posts"
            echo "Accepted: $ACCEPTED"
            echo "Rejected: $REJECTED"
            echo "Errors:   $ERRORS"
          else
            echo "Pipeline output not found"
          fi
          echo "========================================"

      - name: Upload pipeline artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results-${{ github.run_number }}-${{ github.run_attempt }}
          path: |
            data/pipeline-output.json
            scraper/data/xhs/json/*.json
            scraper/data/weibo/json/*.json
          retention-days: 7
