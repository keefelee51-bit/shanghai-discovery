name: Daily Content Pipeline

# Runs daily at 2 AM UTC (10 AM Shanghai time)
# Can also be triggered manually from GitHub Actions tab
on:
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily
  workflow_dispatch:  # Allows manual trigger

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest

    steps:
      # ─────────────────────────────────────────────────────────────
      # STEP 1: Checkout repository (everything in one repo now!)
      # ─────────────────────────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4

      # ─────────────────────────────────────────────────────────────
      # STEP 2: Setup Python for scraper (MediaCrawler)
      # ─────────────────────────────────────────────────────────────
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      # ─────────────────────────────────────────────────────────────
      # STEP 3: Setup Node.js for processing scripts
      # ─────────────────────────────────────────────────────────────
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          

      # ─────────────────────────────────────────────────────────────
      # STEP 4: Install Python dependencies (scraper)
      # ─────────────────────────────────────────────────────────────
      - name: Install scraper dependencies
        working-directory: scraper
        run: |
          pip install -r requirements.txt
          # Install CPU-only torch FIRST to avoid downloading the 2GB CUDA version
          pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install demucs
          echo "✓ Python dependencies installed"

      # ─────────────────────────────────────────────────────────────
      # STEP 4.5: Install Playwright browsers
      # ─────────────────────────────────────────────────────────────
      - name: Install Playwright browsers
        run: |
          playwright install --with-deps chromium
          echo "✓ Playwright browsers installed"

      # ─────────────────────────────────────────────────────────────
      # STEP 5: Install Node.js dependencies (main project)
      # ─────────────────────────────────────────────────────────────
      - name: Install Node.js dependencies
        run: |
          npm ci
          echo "✓ Node.js dependencies installed"

      # ─────────────────────────────────────────────────────────────
      # STEP 5.5: Write Google Vision credentials file
      # ─────────────────────────────────────────────────────────────
      - name: Setup Google Vision credentials
        env:
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
        run: |
          if [ -n "$GOOGLE_CREDENTIALS_JSON" ]; then
            echo "$GOOGLE_CREDENTIALS_JSON" > google-vision-key.json
            echo "✓ Google Vision credentials written"
          else
            echo "⚠️  GOOGLE_CREDENTIALS_JSON not set — image text translation will be skipped"
            echo "{}" > google-vision-key.json
          fi

      # ─────────────────────────────────────────────────────────────
      # STEP 6: Configure scraper with cookies (NO QR CODE NEEDED!)
      # ─────────────────────────────────────────────────────────────
      # STEP 7: Run XHS scraper (cookies passed via env var)
      # ─────────────────────────────────────────────────────────────
      - name: Run XHS scraper
        id: scrape
        working-directory: scraper
        continue-on-error: true
        env:
          COOKIES: ${{ secrets.XHS_COOKIES }}
        run: |
          echo "Starting XHS scraper..."
          python main.py --platform xhs --lt cookie --type search
          echo "✓ XHS scraping completed"
        timeout-minutes: 30

      # ─────────────────────────────────────────────────────────────
      # STEP 7.5: Run Weibo scraper (cookies passed via env var)
      # ─────────────────────────────────────────────────────────────
      - name: Run Weibo scraper
        id: scrape-weibo
        working-directory: scraper
        continue-on-error: true
        env:
          COOKIES: ${{ secrets.WEIBO_COOKIES }}
        run: |
          echo "Starting Weibo scraper..."
          python main.py --platform wb --lt cookie --type search
          echo "✓ Weibo scraping completed"
        timeout-minutes: 20

      # ─────────────────────────────────────────────────────────────
      # STEP 7.7: Verify Weibo data
      # ─────────────────────────────────────────────────────────────
      - name: Verify Weibo data
        id: verify-weibo
        working-directory: scraper
        continue-on-error: true
        run: |
          LATEST=$(ls -t data/weibo/json/search_contents_*.json 2>/dev/null | head -n 1)
          if [ -z "$LATEST" ]; then
            echo "⚠️  No Weibo data found — XHS-only run"
            echo "weibo_file=" >> $GITHUB_OUTPUT
          else
            COUNT=$(node -pe "require('./$LATEST').length")
            echo "✓ Found $COUNT Weibo posts in $LATEST"
            echo "weibo_file=$LATEST" >> $GITHUB_OUTPUT
          fi

      # ─────────────────────────────────────────────────────────────
      # STEP 8: Verify scraped data
      # ─────────────────────────────────────────────────────────────
      - name: Verify scraped data
        id: verify
        working-directory: scraper
        run: |
          echo "Checking for scraped data..."

          # Find the most recent scraped file
          LATEST_FILE=$(ls -t data/xhs/json/search_contents_*.json 2>/dev/null | head -n 1)

          if [ -z "$LATEST_FILE" ]; then
            echo "❌ No scraped data found!"
            exit 1
          fi

          POST_COUNT=$(node -pe "require('./$LATEST_FILE').length")
          echo "✓ Found $POST_COUNT posts in $LATEST_FILE"
          echo "scraped_file=$LATEST_FILE" >> $GITHUB_OUTPUT
          echo "post_count=$POST_COUNT" >> $GITHUB_OUTPUT

      # ─────────────────────────────────────────────────────────────
      # STEP 9: Process posts (Filter & Translate with Claude API)
      # ─────────────────────────────────────────────────────────────
      - name: Process posts with Claude API
        env:
          VITE_ANTHROPIC_API_KEY: ${{ secrets.VITE_ANTHROPIC_API_KEY }}
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
          QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
          GOOGLE_APPLICATION_CREDENTIALS: ./google-vision-key.json
        run: |
          XHS_FILE="./scraper/${{ steps.verify.outputs.scraped_file }}"
          WEIBO_FILE="${{ steps.verify-weibo.outputs.weibo_file }}"

          if [ -n "$WEIBO_FILE" ]; then
            echo "Processing XHS + Weibo posts (limits: 15 XHS, 5 Weibo)..."
            node scripts/process-xhs-posts.mjs "$XHS_FILE" "./scraper/$WEIBO_FILE" --limits 15,5
          else
            echo "Processing XHS-only posts (limit: 15)..."
            node scripts/process-xhs-posts.mjs "$XHS_FILE" --limit 15
          fi

          echo "✓ Processing completed"

      # ─────────────────────────────────────────────────────────────
      # STEP 10: Upload to Supabase
      # ─────────────────────────────────────────────────────────────
      - name: Upload to Supabase
        env:
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
        run: |
          echo "Uploading processed posts to Supabase..."
          node scripts/upload-to-supabase.mjs
          echo "✓ Upload completed"
          
      # ───────────────────────────────────
      # STEP 11: Pipeline Summary
      # ─────────────────────────────────────────────────────────────
      - name: Pipeline Summary
        if: always()
        run: |
          echo "════════════════════════════════════════"
          echo "DAILY PIPELINE SUMMARY"
          echo "════════════════════════════════════════"

          if [ -f "data/pipeline-output.json" ]; then
            ACCEPTED=$(node -pe "require('./data/pipeline-output.json').accepted?.length || 0")
            REJECTED=$(node -pe "require('./data/pipeline-output.json').rejected?.length || 0")
            ERRORS=$(node -pe "require('./data/pipeline-output.json').errors?.length || 0")

            echo "Scraped:  ${{ steps.verify.outputs.post_count }} posts"
            echo "Accepted: $ACCEPTED posts"
            echo "Rejected: $REJECTED posts"
            echo "Errors:   $ERRORS posts"
          else
            echo "⚠️  Pipeline output not found"
          fi

          echo "════════════════════════════════════════"

      # ─────────────────────────────────────────────────────────────
      # STEP 12: Upload artifacts (for debugging)
      # ─────────────────────────────────────────────────────────────
      - name: Upload pipeline results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results-${{ github.run_number }}
          path: |
            data/pipeline-output.json
            scraper/data/xhs/json/*.json
            scraper/data/weibo/json/*.json
          retention-days: 7
