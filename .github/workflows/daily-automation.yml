name: Daily Content Pipeline

# Runs daily at 2 AM UTC (10 AM Shanghai time)
# Can also be triggered manually from GitHub Actions tab
on:
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily
  workflow_dispatch:  # Allows manual trigger

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest

    steps:
      # ─────────────────────────────────────────────────────────────
      # STEP 1: Checkout repository (everything in one repo now!)
      # ─────────────────────────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4

      # ─────────────────────────────────────────────────────────────
      # STEP 2: Setup Python for scraper (MediaCrawler)
      # ─────────────────────────────────────────────────────────────
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      # ─────────────────────────────────────────────────────────────
      # STEP 3: Setup Node.js for processing scripts
      # ─────────────────────────────────────────────────────────────
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      # ─────────────────────────────────────────────────────────────
      # STEP 4: Install Python dependencies (scraper)
      # ─────────────────────────────────────────────────────────────
      - name: Install scraper dependencies
        working-directory: scraper
        run: |
          pip install -r requirements.txt
          echo "✓ Python dependencies installed"

      # ─────────────────────────────────────────────────────────────
      # STEP 5: Install Node.js dependencies (main project)
      # ─────────────────────────────────────────────────────────────
      - name: Install Node.js dependencies
        run: |
          npm ci
          echo "✓ Node.js dependencies installed"

      # ─────────────────────────────────────────────────────────────
      # STEP 6: Configure scraper with cookies (NO QR CODE NEEDED!)
      # ─────────────────────────────────────────────────────────────
      - name: Configure scraper for cookie authentication
        working-directory: scraper
        env:
          XHS_COOKIES: ${{ secrets.XHS_COOKIES }}
        run: |
          echo "Configuring MediaCrawler for cookie authentication..."

          # Update config to use cookie login
          sed -i 's/LOGIN_TYPE = "qrcode"/LOGIN_TYPE = "cookie"/' config/base_config.py

          # Inject cookies from GitHub Secret
          sed -i "s/COOKIES = \"\"/COOKIES = \"$XHS_COOKIES\"/" config/base_config.py

          echo "✓ Cookie authentication configured"

      # ─────────────────────────────────────────────────────────────
      # STEP 7: Run scraper (MediaCrawler) with cookie auth
      # ─────────────────────────────────────────────────────────────
      - name: Run scraper with cookies
        id: scrape
        working-directory: scraper
        continue-on-error: true
        run: |
          echo "Starting MediaCrawler scraper (cookie auth - no QR needed)..."
          python main.py --platform xhs --lt cookie --type search --keywords "上海"
          echo "✓ Scraping completed"
        timeout-minutes: 30

      # ─────────────────────────────────────────────────────────────
      # STEP 8: Verify scraped data
      # ─────────────────────────────────────────────────────────────
      - name: Verify scraped data
        id: verify
        working-directory: scraper
        run: |
          echo "Checking for scraped data..."

          # Find the most recent scraped file
          LATEST_FILE=$(ls -t data/xhs/json/search_contents_*.json 2>/dev/null | head -n 1)

          if [ -z "$LATEST_FILE" ]; then
            echo "❌ No scraped data found!"
            exit 1
          fi

          POST_COUNT=$(node -pe "require('./$LATEST_FILE').length")
          echo "✓ Found $POST_COUNT posts in $LATEST_FILE"
          echo "scraped_file=$LATEST_FILE" >> $GITHUB_OUTPUT
          echo "post_count=$POST_COUNT" >> $GITHUB_OUTPUT

      # ─────────────────────────────────────────────────────────────
      # STEP 9: Process posts (Filter & Translate with Claude API)
      # ─────────────────────────────────────────────────────────────
      - name: Process posts with Claude API
        env:
          VITE_ANTHROPIC_API_KEY: ${{ secrets.VITE_ANTHROPIC_API_KEY }}
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
        run: |
          echo "Processing ${{ steps.verify.outputs.post_count }} posts..."

          # Run processing script with path to scraped data
          SCRAPED_FILE="./scraper/${{ steps.verify.outputs.scraped_file }}"
          node scripts/process-xhs-posts.mjs "$SCRAPED_FILE"

          echo "✓ Processing completed"

      # ─────────────────────────────────────────────────────────────
      # STEP 10: Upload to Supabase
      # ─────────────────────────────────────────────────────────────
      - name: Upload to Supabase
        env:
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
        run: |
          echo "Uploading processed posts to Supabase..."
          node scripts/upload-to-supabase.mjs
          echo "✓ Upload completed"

      # ─────────────────────────────────────────────────────────────
      # STEP 11: Pipeline Summary
      # ─────────────────────────────────────────────────────────────
      - name: Pipeline Summary
        if: always()
        run: |
          echo "════════════════════════════════════════"
          echo "DAILY PIPELINE SUMMARY"
          echo "════════════════════════════════════════"

          if [ -f "data/pipeline-output.json" ]; then
            ACCEPTED=$(node -pe "require('./data/pipeline-output.json').accepted?.length || 0")
            REJECTED=$(node -pe "require('./data/pipeline-output.json').rejected?.length || 0")
            ERRORS=$(node -pe "require('./data/pipeline-output.json').errors?.length || 0")

            echo "Scraped:  ${{ steps.verify.outputs.post_count }} posts"
            echo "Accepted: $ACCEPTED posts"
            echo "Rejected: $REJECTED posts"
            echo "Errors:   $ERRORS posts"
          else
            echo "⚠️  Pipeline output not found"
          fi

          echo "════════════════════════════════════════"

      # ─────────────────────────────────────────────────────────────
      # STEP 12: Upload artifacts (for debugging)
      # ─────────────────────────────────────────────────────────────
      - name: Upload pipeline results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results-${{ github.run_number }}
          path: |
            data/pipeline-output.json
            scraper/data/xhs/json/*.json
          retention-days: 7
