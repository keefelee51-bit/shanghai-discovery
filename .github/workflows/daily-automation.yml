name: Daily Content Pipeline

on:
  # schedule:
  #   - cron: '0 2 */5 * *'  # Uncomment for fully automatic daily runs
  workflow_dispatch:  # Manual trigger from GitHub Actions tab

jobs:
  scrape-and-process:
    runs-on: self-hosted

    defaults:
      run:
        shell: bash

    steps:
      # Windows self-hosted runner: add Git Bash to PATH so all bash steps work
      - name: Add Git Bash to PATH
        shell: cmd
        run: echo C:\Program Files\Git\bin>> %GITHUB_PATH%

      # ══════════════════════════════════════════════════════════════
      # PHASE 1 — Minimum setup needed to run the scraper
      # Goal: get to scraping as fast as possible so a bad cookie
      # fails in ~2 minutes instead of ~12.
      # ══════════════════════════════════════════════════════════════

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      # Install only the scraper's own dependencies (no torch, no demucs yet)
      - name: Install scraper dependencies
        working-directory: scraper
        run: pip install -r requirements.txt

      # Download the Chromium browser binary — required for Playwright scraping
      - name: Install Playwright browser
        run: playwright install --with-deps chromium

      # ══════════════════════════════════════════════════════════════
      # PHASE 2 — Scrape (fail fast if cookies are bad)
      # If XHS cookies are stale the job dies here, saving all the
      # torch / npm install time below.
      # ══════════════════════════════════════════════════════════════

      - name: Run XHS scraper
        id: scrape
        working-directory: scraper
        continue-on-error: true
        env:
          COOKIES: ${{ secrets.XHS_COOKIES }}
        run: |
          echo "Starting XHS scraper..."
          python main.py --platform xhs --lt cookie --type search
          echo "XHS scraping done"
        timeout-minutes: 30

      - name: Run Weibo scraper
        id: scrape-weibo
        working-directory: scraper
        continue-on-error: true
        env:
          COOKIES: ${{ secrets.WEIBO_COOKIES }}
        run: |
          echo "Starting Weibo scraper..."
          python main.py --platform wb --lt cookie --type search
          echo "Weibo scraping done"
        timeout-minutes: 20

      - name: Verify Weibo data
        id: verify-weibo
        working-directory: scraper
        continue-on-error: true
        run: |
          LATEST=$(ls -t data/weibo/json/search_contents_*.json 2>/dev/null | head -n 1)
          if [ -z "$LATEST" ]; then
            echo "No Weibo data — XHS-only run"
            echo "weibo_file=" >> $GITHUB_OUTPUT
          else
            COUNT=$(node -pe "require('./$LATEST').length")
            echo "Found $COUNT Weibo posts in $LATEST"
            echo "weibo_file=$LATEST" >> $GITHUB_OUTPUT
          fi

      # Hard stop here if XHS scraping produced nothing
      - name: Verify XHS scraped data
        id: verify
        working-directory: scraper
        run: |
          LATEST_FILE=$(ls -t data/xhs/json/search_contents_*.json 2>/dev/null | head -n 1)
          if [ -z "$LATEST_FILE" ]; then
            echo "No XHS data found — cookies are likely expired. Update XHS_COOKIES secret and re-run."
            exit 1
          fi
          POST_COUNT=$(node -pe "require('./$LATEST_FILE').length")
          echo "Found $POST_COUNT posts in $LATEST_FILE"
          echo "scraped_file=$LATEST_FILE" >> $GITHUB_OUTPUT
          echo "post_count=$POST_COUNT" >> $GITHUB_OUTPUT

      # ══════════════════════════════════════════════════════════════
      # PHASE 3 — Install processing dependencies
      # Only reached if scraping succeeded, so this time is never
      # wasted on a bad-cookie run.
      # ══════════════════════════════════════════════════════════════

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Node.js dependencies
        run: npm ci

      # CPU-only torch to avoid the 2 GB CUDA download
      - name: Install audio processing dependencies
        working-directory: scraper
        run: |
          pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install demucs

      - name: Setup Google Vision credentials
        env:
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
        run: |
          if [ -n "$GOOGLE_CREDENTIALS_JSON" ]; then
            echo "$GOOGLE_CREDENTIALS_JSON" > google-vision-key.json
            echo "Google Vision credentials written"
          else
            echo "GOOGLE_CREDENTIALS_JSON not set — image text translation skipped"
            echo "{}" > google-vision-key.json
          fi

      # ══════════════════════════════════════════════════════════════
      # PHASE 4 — Process and upload
      # ══════════════════════════════════════════════════════════════

      - name: Process posts with Claude API
        env:
          VITE_ANTHROPIC_API_KEY: ${{ secrets.VITE_ANTHROPIC_API_KEY }}
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
          QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}
          GOOGLE_APPLICATION_CREDENTIALS: ./google-vision-key.json
        run: |
          XHS_FILE="./scraper/${{ steps.verify.outputs.scraped_file }}"
          WEIBO_FILE="${{ steps.verify-weibo.outputs.weibo_file }}"

          if [ -n "$WEIBO_FILE" ]; then
            echo "Processing XHS + Weibo posts (limits: 15 XHS, 5 Weibo)..."
            node scripts/process-xhs-posts.mjs "$XHS_FILE" "./scraper/$WEIBO_FILE" --limits 15,5
          else
            echo "Processing XHS-only posts (limit: 15)..."
            node scripts/process-xhs-posts.mjs "$XHS_FILE" --limit 15
          fi

      - name: Upload to Supabase
        env:
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
        run: node scripts/upload-to-supabase.mjs

      # ══════════════════════════════════════════════════════════════
      # ALWAYS — Summary + artifacts (runs even on failure)
      # ══════════════════════════════════════════════════════════════

      - name: Pipeline Summary
        if: always()
        run: |
          echo "========================================"
          echo "PIPELINE SUMMARY"
          echo "========================================"
          if [ -f "data/pipeline-output.json" ]; then
            ACCEPTED=$(node -pe "require('./data/pipeline-output.json').accepted?.length || 0")
            REJECTED=$(node -pe "require('./data/pipeline-output.json').rejected?.length || 0")
            ERRORS=$(node -pe "require('./data/pipeline-output.json').errors?.length || 0")
            echo "Scraped:  ${{ steps.verify.outputs.post_count }} posts"
            echo "Accepted: $ACCEPTED"
            echo "Rejected: $REJECTED"
            echo "Errors:   $ERRORS"
          else
            echo "Pipeline output not found"
          fi
          echo "========================================"

      - name: Upload pipeline artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results-${{ github.run_number }}-${{ github.run_attempt }}
          path: |
            data/pipeline-output.json
            scraper/data/xhs/json/*.json
            scraper/data/weibo/json/*.json
          retention-days: 7
